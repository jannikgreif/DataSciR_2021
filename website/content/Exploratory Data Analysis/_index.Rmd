---
title: Exploratory Data Analysis
weight: 4
chapter: true
---

<style>#chapter p {
text-align: justify} #chapter h3 {
text-align: left}
</style>

## 4. Exploratory Data Analysis

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(stringr)
library(ggpubr)
knitr::opts_chunk$set(
  echo = FALSE, 
  eval = FALSE, 
  message = FALSE,
  warning = FALSE, 
  fig.show = "asis"
)
data_dir <- "../../../data"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")
sentiments_dir <- data_dir %>% paste("sentiments", sep = "/")

player_metadata <- paste(data_dir,"player-metadata.csv", sep ="/") %>% read_csv()
player_game_stats <- paste(data_dir,"player-game-stats.csv", sep ="/") %>% read_csv()
game_metadata <- paste(data_dir,"game-metadata.csv", sep ="/") %>% read_csv()
player_season_stats <- paste(data_dir,"player-season-stats.csv", sep ="/") %>% read_csv()

tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)
prep_tweets <- data_dir %>% 
  paste("prep-tweets.csv", sep = "/") %>% 
  read_csv()

sentiment_names <- c("bing", "syuzhet", "jockers_rinker", "afinn", "nrc", "novak_emoji")
sentiments <- sentiment_names %>% 
  map_dfr(~ {
    sentiment_name <- .x
    
    sentiment_file <- sentiments_dir %>% 
      paste(sentiment_name, sep = "/") %>% 
      paste0(".csv") %>% 
      str_replace("_", "-")
    
    sentiments_tmp <- read_csv(sentiment_file) %>% 
      mutate(positive_sentiment = if_else(ave_sentiment >= 0, TRUE, FALSE)) %>% 
      mutate(sentiment_lexicon = sentiment_name)
  })
```

### 4.1 Comparability of the Sentiment Lexicons

In the beginning we wanted to assess if the different lexica sentiment scores for tweets were actually comparable. For that purpose we computed the Spearman rank correlation coefficient between tweet sentiment scores provided by each pair of sentiment lexica to assess whether the ranking of the tweets according to one sentiment lexicon agrees with the ranking of the tweets according to another sentiment lexicon.

```{r compute_sentiment_consensus_spearman, eval = TRUE, include = FALSE}
sentiment_lexicons <- sentiments$sentiment_lexicon %>% unique()
# Create cross product of sentiment_lexicon with itself
sentiment_lexicons_a <- sentiment_lexicons %>% 
  rep(each = length(sentiment_lexicons))
sentiment_lexicons_b <- sentiment_lexicons %>% 
  rep(times = length(sentiment_lexicons))
# For each pair of sentiment lexicons compute Kendall rank correlation coefficient
sentiment_lexicon_consensus <- map2_dfr(sentiment_lexicons_a, sentiment_lexicons_b, ~ {
  
  sentiments_a <- sentiments %>% 
    filter(sentiment_lexicon == .x) %>% 
    mutate(ave_sentiment_a = ave_sentiment) %>% 
    select(id, ave_sentiment_a) 
  
  sentiments_b <- sentiments %>% 
    filter(sentiment_lexicon == .y) %>%
    mutate(ave_sentiment_b = ave_sentiment) %>% 
    select(id, ave_sentiment_b)
  
  inner_join(sentiments_a, sentiments_b) %>% 
    mutate(sentiment_lexicon_a = .x) %>% 
    mutate(sentiment_lexicon_b = .y) %>%  
    group_by(sentiment_lexicon_a, sentiment_lexicon_b) %>% 
    summarise(consensus = cor(ave_sentiment_a, ave_sentiment_b, method = "spearman", use = "complete.obs")) %>% 
    ungroup()
})
```

```{r sentiment_corr, eval = TRUE, fig.width = 10, echo = FALSE}
sentiment_lexicon_consensus %>% 
  ggplot(mapping = aes(x = sentiment_lexicon_a, y = sentiment_lexicon_b)) +
    geom_tile(mapping = aes(fill = consensus)) +
    geom_text(mapping = aes(label = round(consensus, 3))) +
    scale_fill_gradient(low = "white", high = "red") + 
    labs(
      title = "Consensus of the Sentiment Lexicons in giving Sentiment\nScores to Tweets addressed to NBA Basketball Players", 
      subtitle = "Using the Spearman Rank Correlation Coefficient"
    ) +
    theme_minimal() + 
    theme(
      plot.title = element_text(hjust = 0.5), 
      plot.subtitle = element_text(hjust = 0.5),
      axis.title.x = element_blank(),
      axis.title.y = element_blank()
    )
```


### 4.2 Computing Sentiment Aggregates

Since the sentiment scores were computed on a per-tweet basis we first had to aggregate the sentiment scores accordingly in order to capture the overall social media vibe, players were receiving. For that purpose we considered  the sentiment scores of all tweets a player received in a 24 hour window before a game and aggregated them as follows:  

  - The average of the sentiment scores (mean)
  - The average of the sentiment scores weighted by the retweet count of the associated tweet (weighted mean)
  - The proportion of tweets with a negative associated sentiment score (< 0)
```{r compute_sentiment_aggregates_24h_before_games, eval = TRUE, include = FALSE}
tweets_24h_before_games <- player_game_stats %>%
  inner_join(game_metadata) %>% 
  inner_join(tweets) %>% 
  mutate(h_timediff_game = as.double(DateTime - created_at, units = "hours")) %>% 
  select(c(names(tweets), h_timediff_game))  %>%
  filter(h_timediff_game <= 24)
sentiment_aggregates_24h_before_games <- tweets_24h_before_games %>% 
  inner_join(sentiments) %>% 
  group_by(BBRef_Player_ID, BBRef_Game_ID, sentiment_lexicon) %>% 
  summarise(
    avg_sentiment = mean(ave_sentiment),
    avg_sentiment_retweet_cnt_weighted = weighted.mean(ave_sentiment, retweet_count),
    rel_freq_negative = sum(!positive_sentiment) / n()
  ) 
```


### 4.3 Distribution of the Sentiment Aggregates and BPM
Looking at the individual density curves we observed that the distributions of the average sentiment scores rougly fit the bell curve of a Normal distribution despite a few exceptions (esp. for the averaged sentiments for the emoji sentiment lexicon by Novak). 
We also studied how the BPM perfomance indicator values are distributed for the different players. Similar to the unweighted and weighted sentiment averages before, BPM was also normally distributed.

Knowing that the BPM values were normally distributed for the different players, it was sufficient to simply construct boxplots for the performance indicator to get a sense how the individual players performed in general in the two considered seasons and how their performance fluctuated. 

```{r plot-boxplot-BPM, eval = TRUE, fig.width = 10, echo = FALSE}
player_metadata %>% 
  inner_join(player_game_stats) %>% 
  inner_join(sentiment_aggregates_24h_before_games) %>% 
  distinct(Player, BBRef_Game_ID, BPM) %>% 
  ggplot(mapping = aes(x = reorder(Player, BPM, na.rm = TRUE), y = BPM)) + 
    geom_boxplot() + 
    coord_flip() +
    labs(
      title = "Boxplots of the Box Plus/Minus (BPM) Performance Indicator",
      subtitle = "For different Players between the Seasons 2017-18 and 2018-19", 
      y = "Box Plus/Minus (BPM)"
    ) + 
    theme(
      plot.title = element_text(hjust = 0.5), 
      plot.subtitle = element_text(hjust = 0.5),
      axis.title.y = element_blank()
    ) 
```

### 4.4 Relationship between the Average 24-Hour Tweet Sentiment and the BPM Performance Indicator

We included a RShiny App to showcase each players correlation between BPM performance and each tweet sentiment. Starting up the app may take some time. 

<iframe src="https://jannikgreif.shinyapps.io/DataSciR_correlation/" width="100%" height="650"></iframe>

As one can see, the points of the different scatterplots appeared rather scattered. For the different sentiment 
lexicons and players there was neither a strong nor directly visible (linear) relationship between the average tweet 
sentiment and the BPM performance indicator. Even though some of the linear regression lines suggested a correlation, the
correlations themselves were rather weak or even neglectable as indicated by the respective Pearson correlation 
coefficients *r* that were relatively small (mostly less than 0.1). Additionally most of the *p*-values of the associated
Pearson correlation coefficients were rather high which suggested that the observed strength of the correlations were not
significantly different from 0 (and might have appeared due to random chance).

Nevertheless, there were also some counter examples where the Pearson correlation coefficient appeared rather 
significant. The player Jaylen Brown for example showed a positive correlation for the Afinn lexicon with a *p*-value 
below 0.05. However, since the correlations were rather weak, not significant and somehow contradicting for other 
sentiment lexicons (compare that the correlation was negative for the nrc lexicon), it is debatable if the positive 
correlation is generalizable for the entire population or even the single player alone. 

Due to these reasons we had to conclude that there is no evidence of a significantly strong linear correlation between 
the average sentiment of tweets players receive within 24 hours before games and their performance within the games. 

There was however another interesting observation the scatterplots revealed, namely the prominent outliers. For almost 
every player there was at least one game day in which the average tweet sentiment was vastly more positive compared to 
other days. Additionally there were some players with game days associated with an extremely negative average tweet 
sentiment in comparison to other days. To investigate these outliers more closely we created two word clouds for each 
player, one for the smallest average tweet sentiment the player received and one for the highest. We used the tweet 
sentiments created from the Jockers-Rinker lexicon for this purpose. On extremely positive sentiment values (outliers) the player had birthday.

This RShiny App showcases the wordclouds for each player. 
<iframe src="https://jannikgreif.shinyapps.io/DataSciR_wordclouds/" width="100%" height="650"></iframe>

