---
title: The Impact of NBA player-related Social Media Posts on their on-court Performance
  - An Analysis
author: "Frank Dreyer, Kolja Günther, Jannik Greif"
date: "20.05.2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
subtitle: DataSciR - Project Notebook
bibliography: references.bib
csl: ieee.csl
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center} \includegraphics[width=2in,height=2in]{"datascir.png"}\LARGE\\}
- \posttitle{\end{center}}
link-citations: yes
---

```{r setup, include=FALSE, eval = TRUE}
knitr::opts_chunk$set(echo = TRUE)
# get-tweets packages
library(tidyverse)
library(rtweet)
library(academictwitteR)
library(anytime)
# nba-stats packages
library(rvest)
library(naniar)
# pre-processing packages
library(stringr)
library(textclean)
# sentiment extraction packages
library(magrittr) # for %$% operator
library(sentimentr)
# analysis packages
library(ggpubr)
# predictive model packages
library(lubridate)
library(purrr)
library(rvest)
library(ranger)
library(tidymodels)

knitr::opts_chunk$set(
  echo = FALSE, 
  eval = FALSE, 
  message = FALSE,
  warning = FALSE,
  fig.show = "asis"
)

data_dir <- "../data"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")
sentiments_dir <- data_dir %>% paste("sentiments", sep = "/")

# Twitter Api Credentials
twitter_app_name <- "DataSciR"
twitter_api_key <- "he8beW6mFPz12r8QiqtXsZbLN" # Insert API key here
twitter_api_secret_key <- "yFnZfhVitCl9fWixlImG6BRGkCX7c2HjlggHv3HgSYtnJIDokX" # Insert API key secret here
twitter_access_token <- "1397627861503287305-8YHK3ANCzUt0x0Ufdb1OICJ75i4fns" # Insert access token here
twitter_access_token_secret <- "cbK5fyPzVLtczRmWnMiDuZOLrvSaJldaSDQRZ31Hubgki" # Insert access token secret here
twitter_bearer_token <- "AAAAAAAAAAAAAAAAAAAAAMJSQAEAAAAAxo6ezvCA34%2F%2B1gBbknootwxLdqs%3DtP8sqsQarCLEhnQXuFYXN79PxWfpG0btbSvDmBEZfePpM23Zyy" # Insert bearer-token here

twitter_token <- create_token(
  app = twitter_app_name,
  consumer_key = twitter_api_key,
  consumer_secret = twitter_api_secret_key, 
  access_token = twitter_access_token,
  access_secret = twitter_access_token_secret
)
```

\newpage

## Github Repository

The project is documented at: https://github.com/jannikgreif/DataSciR_2021


## Team Members

```{r team, eval = TRUE}
library(knitr)
library(kableExtra)
team <- data.frame(Name = c("Jannik Greif","Kolja Günther","Frank Dreyer"),
                   `Course of Studies` =c("M.Sc. Wirtschaftsinformatik","M.Sc. Data and Knowledge Engineering","M.Sc. Data and Knowledge Engineering"),
                  Mail = c("jannik.greif@st.ovgu.de","kolja.guenther@st.ovgu.de","frank.dreyer@st.ovgu.de"), check.names = FALSE)
kbl(team, booktabs = T, linesep = "") %>%
  kable_styling(position = "center")%>%
  kable_styling(latex_options = "HOLD_position")
```

## 1. Overview
The present project aims to discover a significant impact of social media posts addressed to NBA players before matches with respect to their influence on these players’ in-game performance. For this purpose, we considered NBA players that are highly active on Twitter and extract tweets that are addressed to them within a short period of time before matches via the Twitter API. A sentiment analysis was then applied to indicate the attitude of the posts and with the resulting sentiment polarity scores we tested if there is a correlation between social media posts and players’s on-court performance.

## 2. Motivation and Related Work
With the growing presence of social media in all areas of life, allowing people from around the world to react to current events in real-time, an increasingly controversial discussion can be noticed. Today more than ever, public figures are exposed to the reactions of millions of people, observing and commenting on every step in their life that becomes public. The resulting negative impact that extensive social media usage can have on users' behavior and mental state is subject to different scientific studies [-@kapoor_advances_2018;-@berryman_social_2018]. 
 
Sports athletes, who use social media not only to communicate with peers and fans but also to promote themselves, are no exception to these issues [-@academy_does_2008]. Among researchers in the sports field, there is a consensus that the mental state of an athlete can have a significant impact on his or her performance [-@xu_measuring_2015]. However, only little research has been conducted in order to analyze if and how social media usage of athletes directly influences their performance.
Xu and Yu [-@xu_measuring_2015] tried to capture the mood of basketball players in the NBA from the tweets they posted just before a match, using sentiment analysis, to analyze how the predicted mood influenced their performance on court. Gruettner, Vitisvorakarn and Wambsganss [-@gruttner_new_2020] used a similar approach on tennis players and additionally analyzed the relationship between the number of tweets they posted before matches and their performance within the match. Even though both contributions show that athletes with a bad predicted mood tend to perform worse on-court, they suffer from two limitations: 
 
  1.    The number of tweets an athlete posts per day is rather limited 
  2.    The predicted moods are not free of bias since an athlete might only post tweets how he or she wants to be seen on Twitter (also indicated in         [-@gruttner_new_2020])
 
Both of these limiting factors may lead to an inaccurate prediction of the mental state of athletes. \newline
We believe that the attitude of social media posts an athlete receives from peers and fans is also a good predictor for his or her performance. Ott and Puymbroeck support this claim  [-@academy_does_2008]. In their article they list cases where athletic performance appeared to be immediately influenced by the media and conclude that the media has the potential to change the performance of an athlete in a negative as well as positive way. In this analysis we aim to assess this relationship more closely by analyzing how social media posts addressed to NBA players affect their in-game performance.

## 3. Initial Research Questions and Project Objectives
In the beginning of our project we want to give a brief overview on our research goal and the tasks we want to fulfill.\newline
This project aims to answer the following research question: \newline
**Can we find a significant correlation between negative/positive Social Media posts related to a specific NBA player and his on-court performance in the following game?**\newline

To answer this research question we worked on the following objectives:\newline

**Objective 1: Dataset Creation** \newline
Acquire game statistics of NBA players that are highly active on Twitter and the tweets they received from peers and fans in an appropriate time window before games. The game statistics should include an appropriate metric that describes how the player performed within a corresponding game. The tweets need to be preprocessed accordingly to have them in an appropriate format in order to use them for further analysis steps. The attitude of the extracted posts should be captured by assigning a sentiment score to them. The sentiment scores of the tweets a player received in the corresponding time window before a game should be aggregated accordingly and linked to the respective game. As a result, this should end in a data set in which each record contains the game statistics of a player for a specific game and the aggregated sentiment information of the tweets that were addressed to the player before the game. 

**Objective 2: Exploratory Data Analysis** \newline
Analyze the association between the aggregated polarity scores of the tweets a player received before games and the performance of the player within the games using appropriate performance metrics. Additionally, the strength and significance of the correlation should be evaluated. 

**(Additional objective 3: Prediction Model ** \newline
After we investigated the results of our analysis we decided to additionally set up a prediction model to check the findings we made. With this model we wanted to explore the influence of tweets's sentiments on a prediction task of a NBA player's on-court performance. For this purpose we fed our sentiments as features into the predictor.

## 4. The Data
The main challenge in the pre-processing phase of our project was to create suitable datasets which reflect both of the variables we wanted to include into our analysis. For the player's performance variable we needed to create a set of datasets which cover all necessary statistics and metadata to be able to derive the needed values. For the sentiment variable of the tweets referring to one respective game and player, we first needed to narrow down the selection of players whose tweets we wanted to observe and then extract all tweets that are related to this set of players. How this was done is described in the following section.

### 4.1 NBA Stats Datasets

#### 4.1.1 Introduction

To get the needed data about players, games, seasons and all relevant metadata, we extracted statistical datasets from [basketball-reference.com](https://basketball-reference.com), a site which provides historical basketball statistics of players and teams from various US American and European leagues including the NBA. From this we created local .csv files for different metrics.

#### 4.1.2 Setup & Extracting Player Metadata
To get started, we set up our environment and for extracting the data from [basketball-reference.com](https://basketball-reference.com) we extensively used the web scraping library `rvest` in addition to the `tidyverse`.

Before extracting stats about NBA players and games, we had to check, which players even have a twitter account. Fortunately for us, [basketball-reference.com](https://basketball-reference.com) provides a list of [Twitter usernames of NBA players](https://www.basketball-reference.com/friv/twitter.html), so we loaded the account names into the player-metadata.csv, along with an unique BBRef_Player_ID, which we took over from [basketball-reference.com](https://basketball-reference.com), and the clear name of the respective players.
With this set of players we now wanted to extract further statistics.

```{r player-metadata}
url <- "https://www.basketball-reference.com/friv/twitter.html"

metadata_tbl <- read_html(url) %>% 
  html_element("table.stats_table") 

player_td = metadata_tbl %>% 
  html_elements("td[data-stat=\"player\"]") 

twitter_td = metadata_tbl %>% 
  html_elements("td[data-stat=\"twitter\"]")

BBRef_Player_IDs = player_td %>% 
  html_element("a") %>% 
  html_attr("href") %>% 
  map_chr(~ str_extract(.x, "[a-z]/[a-z]+[0-9]{2}"))

player_names = player_td %>% html_text(trim = TRUE)

twitter_names = twitter_td %>% html_text(trim = TRUE)

metadata <- tibble(
  BBRef_Player_ID = BBRef_Player_IDs,
  Player = player_names,
  Twitter = twitter_names
)

file_path <- data_dir %>% paste("player-metadata.csv", sep = "/")
metadata %>% write_csv(file_path)
```

```{r show-player-metadata, eval = TRUE}
player_metadata <- data_dir %>% 
  paste("player-metadata.csv", sep = "/") %>% 
  read_csv()

player_metadata %>%
  head(100)
```

#### 4.1.3 Getting Player Season Statistics
The idea behind this dataset was to create a tibble which included all the statistics of players on season-level. As a basketball season is split into a regular season (comparable to our "Bundesliga"-system) and a playoff season (comparable to a tournaments k.o.-phase) which only the best teams of one regular season can pass, [basketball-reference.com](https://basketball-reference.com) provides two separate datasets, one for each season type.
To combine those datasets and map them to each player in one table, we set up loops, that check for each season, whether a player actively participated in either the regular season and/or the playoffs, extract the statistics if the condition holds true and tag each tuple of either the regular season statistics or the playoff statistics with a respective label "Regular Season" or "Playoffs". If one player didn't participate in any of the both possibilities, we set all variables of this entry to NA. This step is necessary as the original data labels each variable entry with a respective string, like "Did not play" or "Inactive"
Finally the dataset contained one tuple of statistics for each player and season/seasontype he participated in, including the following metrics:

```{r basketball-reference, echo = FALSE}
library(knitr)
table <- data.frame(
  Attribute = c("Starters / Reserves","MP","FG","FGA","FG%","3P","3PA","3P%","FT","FTA","FT%","ORB","DRB","TRB","AST","STL","BLK","TOV","PF","PTS","+/-"),
  `Data Type` = c("String","Timediff","Int","Int","Float","Int","Int","Float","Int","Int","Float",rep("Int",times=10)),
  Description = c("Name of player (separated in starters and reserves)","Minutes Played","Field Goals: number made shots (excluding free throws)","Field Goal Attempts = number of shot attempts (excluding free throws)","Field Goal Percentage: fraction of field goal attempts (FG/FGA)","3-Point Field Goals: number of made 3-point shots","3-Point Field Goal Attempts: number of 3-point shot attempts","3-Point Field Goal Percentage: fraction of three point shot attempts (3P/3PA)","Free Throws: number of free throw shots ","Free Throw Attempts: number of free throw shot attempts","Free Throw Percentage: fraction of free throw attempts (FT/FTA)","Offensive Rebounds","Defensive Rebounds","Total Rebounds (ORB+TRB)","Assists","Steals","Blocks","Turnovers","Personal Fouls","Points made","Estimates the players’ contribution to the team when the player is on the court"), check.names = FALSE
)
kbl(table, booktabs = T, linesep = "") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position") %>%
  kable_styling(latex_options = "striped")
```

```{r player-season-stats}
get_player_season_stats <- function(BBRef_Player_ID) {
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  html_regular_season_tbl <- html %>% html_node("table#per_game")
  html_playoffs_tbl <- html %>% html_node("table#playoffs_per_game")
  
  season_stats <- NULL
  regular_season_stats <- NULL
  playoffs_season_stats <- NULL
  
  # If player participated in regular season
  if (!is.na(html_regular_season_tbl)) {
    regular_season_stats <- html_regular_season_tbl %>% 
      html_table(trim = TRUE, convert = FALSE) %>% 
      mutate(SeasonType = "Regular Season", .after = Season)
  }
  
  # If player participated in playoffs
  if (!is.na(html_playoffs_tbl)) {
    playoffs_season_stats <- html_playoffs_tbl %>% 
      html_table(trim = TRUE, convert = FALSE) %>% 
      mutate(SeasonType = "Playoffs")
  }
  
  # If player participated in regular season or playoffs
  if (! is.null(regular_season_stats) | ! is.null(playoffs_season_stats)){
    season_stats <- regular_season_stats %>%
      bind_rows(playoffs_season_stats) %>% 
      filter(str_detect(Season, "[0-9]{4}-[0-9]{2}")) %>%   
      mutate(BBRef_Player_ID = BBRef_Player_ID, .before = Season) %>% 
      naniar::replace_with_na_all(
        condition = ~ str_detect(.x, "Did Not Play")
      ) %>% 
      type.convert(as.is = TRUE)
  }
  
  season_stats
  
}

player_season_stats <- player_metadata$BBRef_Player_ID %>% 
  map_dfr(get_player_season_stats) %>% 
  mutate(Team = if_else(! is.na(Tm), Tm, Team)) %>% 
  select(-Tm)


file_path <- data_dir %>% paste("player-season-stats.csv")
player_season_stats %>% write_csv(file_path)
```

```{r load-player-season-stats, eval = TRUE}
player_season_stats <- data_dir %>% 
  paste("player-season-stats.csv", sep = "/") %>% 
  read_csv()
```

```{r display-player-season-stats, eval = TRUE, message = TRUE}
player_season_stats %>%
  head(100)
```

#### 4.1.4 Extracting Player Game Statistics
The next step was to extract performance statistics of the NBA-players on game-granularity. With this we wanted to create our main source for the performance indicators, which we wanted to exploit for our exploratory analysis.
According to [Wikipedia](https://en.wikipedia.org/wiki/Twitter) Twitter was found in 2006. Probably not many NBA players had a Twitter account during that time. In 2007 only 400,000 tweets were posted per quarter. However, the popularity of Twitter skyrocketed after its founding with over 50 million daily tweets in 2010. Let's therefore only consider players that actively played from 2010 onward. 
Since player performance metrics like +/- become rather unreliable if a player only gets a small amount of playing time, we only considered players that on average get at least two quarters of playing time (i.e. 24 minutes).

```{r filter-relevant-players, eval = TRUE}

min_MP <- 24
min_Season <- "2010"

relevant_players <- player_season_stats %>% 
  filter(Season >= min_Season) %>% 
  group_by(BBRef_Player_ID) %>% 
  summarise(
    AVG_MP = mean(MP, na.rm = TRUE)
  ) %>% 
  ungroup() %>% 
  filter(AVG_MP >= min_MP) 

```

```{r display-relevant-player, eval = TRUE, message = TRUE}
relevant_players %>%
  head(100)
```

In detail the player-game-stats.csv contains for each player and for each game within our observation range a detailed set of metadata, like the season type and date of the game or the team for which the player started, as well as individual performance statistics for each game, like the ones we also extracted for the season statistics but on game-granularity plus extra metrics that can be obtained for each game individually. While the already known statistics were obtained from the basic game logs, [basketball-reference.com](https://basketball-reference.com) offers for each individual player, the additional data was extracted from the advanced game logs. With these advanced statistics we also added the Box Plus/Minus score, "a box score estimate of the points per 100 possessions a player contributed above a league-average player (defined as being 0.0), translated to an average team."[TODO reference!]
This metric is calculated from the box score information of a player, his position on-court, and the overall performance of the team. Explained on an example, a score of +10.0 would mean, that the overall team is 10 points per 100 possessions better with this particular player on-court than with an average player. It should become the central metric of player performance for our correlation analysis.

```{r player-game-stats}

not_played_keys <- c(
  "Did Not Play", 
  "Did Not Dress", 
  "Inactive",
  "Not With Team",
  "Player Suspended"
)


get_player_game_stats <- function(BBRef_Player_ID) {
  
  print(BBRef_Player_ID)
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  player_seasons <- get_player_seasons(BBRef_Player_ID)
  BBRef_Player_ID_rep <- rep(BBRef_Player_ID, times = player_seasons %>% length())
  
  basic_gamelogs <- map2_dfr(
    BBRef_Player_ID_rep, player_seasons, ~ get_gamelogs(.x, .y, "basic")
  ) 
  
  advanced_gamelogs <- map2_dfr(
    BBRef_Player_ID_rep, player_seasons, ~ get_gamelogs(.x, .y, "advanced")
  )
  
  gamelogs <- inner_join(basic_gamelogs, advanced_gamelogs) %>% 
    type.convert()
  
  gamelogs
  
}


get_player_seasons <- function(BBRef_Player_ID) {
  
  url <- glue::glue("https://www.basketball-reference.com/players/{BBRef_Player_ID}.html")
  html <- read_html(url)
  
  seasons <- html %>% 
    html_elements("th[data-stat=\"season\"]") %>% 
    html_element("a") %>% 
    html_text() %>% 
    unique() %>% 
    na.omit()
  
  seasons
  
}


get_gamelogs <- function(BBRef_Player_ID, season, gamelog_type) {
  
  url <- glue::glue(
    "https://www.basketball-reference.com/players/{id}/gamelog{type}/{s}",
    id = BBRef_Player_ID,
    type = if_else(gamelog_type == "basic", "", paste("", gamelog_type, sep = "-")),
    s = season %>% 
      str_remove("[0-9]{4}-") %>% 
      paste("0101", sep = "") %>% 
      lubridate::ymd() %>% 
      lubridate::year()
  )
  html <- read_html(url)
  
  print(url)
  
  html_regular_season_gamelog_tbl <- html %>%  
    html_node(glue::glue("table#pgl_{gamelog_type}"))
  
  # Playoffs game logs embedded in HTML comment
  html_playoffs_gamelog_tbl <- html %>% 
    html_nodes(xpath = '//comment()') %>% 
    html_text() %>% 
    paste(collapse = '') %>% 
    read_html() %>% 
    html_node(glue::glue("table#pgl_{gamelog_type}_playoffs"))
  
  regular_season_gamelogs <- NULL
  playoffs_gamelogs <- NULL
  gamelogs <- NULL
  
  # If player participated in regular season game
  if (!is.na(html_regular_season_gamelog_tbl)) {
    regular_season_gamelogs <- html_regular_season_gamelog_tbl %>% 
      html_table(trim = TRUE, convert = FALSE, header = NA) 
    
    names(regular_season_gamelogs)[6] <- "HTm"
    names(regular_season_gamelogs)[8] <- "WL"
    
    regular_season_gamelogs <- regular_season_gamelogs %>% 
      mutate(SeasonType = "Regular Season", .before = Date)
  }
  
  # If player participated in playoffs game 
  if (!is.na(html_playoffs_gamelog_tbl)) {
    playoffs_gamelogs <- html_playoffs_gamelog_tbl %>% 
      html_table(trim = TRUE, convert = FALSE, header = NA)
    
    names(playoffs_gamelogs)[6] <- "HTm"
    names(playoffs_gamelogs)[8] <- "WL"
    
    playoffs_gamelogs <- playoffs_gamelogs %>% 
      mutate(SeasonType = "Playoffs", .before = Date)
  }
  
  # If player participated in regular season or playoffs
  if (!is_null(regular_season_gamelogs) | !is_null(playoffs_gamelogs)) {
    gamelogs <- regular_season_gamelogs %>%
      bind_rows(playoffs_gamelogs) %>% 
      filter(Date != "Date") %>%      # Filter out header rows
      mutate(Season = season, .before = SeasonType) %>% 
      mutate(BBRef_Player_ID = BBRef_Player_ID, .before = Season) %>% 
      mutate(HTm = if_else(HTm == "@", Opp, Tm)) %>% 
      naniar::replace_with_na_all(~ .x %in% not_played_keys) %>% 
      mutate(BBRef_Game_ID = map2_chr(
        Date, HTm, 
        ~ paste(lubridate::ymd(.x) %>% format("%Y%m%d"), .y, sep = "0")
      ), .after = BBRef_Player_ID) %>% 
      type.convert(as.is = TRUE)
  }
  
  gamelogs
  
}


get_game_time <- function(game_url) {
  
  read_html(game_url) %>% 
    html_element("div.scorebox_meta") %>% 
    html_text() %>% 
    str_extract("[0-9]{1,2}:[0-9]{2} [A|P]M")
    
}


subselection <- relevant_players %>% head()

player_game_stats <- relevant_players$BBRef_Player_ID %>%
  map_dfr(get_player_game_stats)

file_path <- data_dir %>% paste("player-game-stats.csv", sep = "/")
player_game_stats %>% write_csv(file_path)
```

```{r load-player-game-stats, eval = TRUE}
player_game_stats <- data_dir %>% 
  paste("player-game-stats.csv", sep = "/") %>% 
  read_csv()
```

```{r display-player-game-stats, eval = TRUE, message = TRUE}
player_game_stats %>%
  head(100)
```

#### 4.1.5 Creating Game Metadata
The last data source we wanted to create, was a table of metadata for each game. For this purpose we extracted the NBA schedule and results from [basketball-reference.com](https://basketball-reference.com) for each season from 2010 to 2021 column-wise and merged these columns into one tibble. This was then stored in the game-metadata.csv.

```{r game-metadata, eval = FALSE}

get_game_metadata <- function(season) {
  
  year <- season %>% str_remove("[0-9]{4}-") %>% 
    paste("0101", sep = "") %>% 
    lubridate::ymd() %>% 
    format("%Y")
  
  url <- glue::glue("https://www.basketball-reference.com/leagues/NBA_{year}_games.html")
  html <- read_html(url)
  
  game_metadata <- html %>% 
    html_element("div.filter") %>%
    html_elements("a") %>% 
    html_attr("href") %>% 
    map_dfr(get_game_schedule)
  
  game_metadata
  
}


get_game_schedule <- function(url) {
  
  url <- glue::glue("https://www.basketball-reference.com{url}")
  html <- read_html(url)
  
  html_schedule_tbl <- html %>% html_node("table#schedule")
  
  game_dates <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("th[data-stat=\"date_game\"]") %>% 
    html_element("a") %>% 
    html_text2() %>% 
    lubridate::mdy()
  
  game_start_times <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"game_start_time\"]") %>% 
    html_text2() %>% 
    paste0("m")
  
  game_home_teams <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"home_team_name\"]") %>% 
    html_attr("csk") %>% 
    str_sub(start = 1, end = 3)
  
  game_home_pts <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"home_pts\"]") %>% 
    html_text2()
  
  game_visit_teams <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"visitor_team_name\"]") %>% 
    html_attr("csk") %>% 
    str_sub(start = 1, end = 3)
  
  game_visit_pts <- html_schedule_tbl %>% 
    html_element("tbody") %>% 
    html_elements("td[data-stat=\"visitor_pts\"]") %>% 
    html_text2()

  schedule <- tibble(
    Date = game_dates,
    Start = game_start_times,
    HomeTm = game_home_teams,
    HomePTS = game_home_pts,
    VisitTm = game_visit_teams,
    VisitPTS = game_visit_pts
  ) %>% 
    mutate(DateTime = map2_chr(Date, Start, paste), .before = Date) %>% 
    mutate(DateTime = lubridate::ymd_hm(DateTime)) %>% 
    mutate(BBRef_Game_ID = map2_chr(
      Date, HomeTm, ~ format(.x, "%Y%m%d") %>% paste(.y, sep = "0")
    ), .before = DateTime)
  
}

game_metadata <- player_season_stats %>% 
  filter(Season >= min_Season) %>% 
  pull(Season) %>% 
  unique() %>% 
  map_dfr(get_game_metadata)

file_path <- data_dir %>% paste("game-metadata.csv", sep = "/")
game_metadata %>% write_csv(file_path)

```

```{r load-game-metadata, eval = TRUE}
game_metadata <- data_dir %>% 
  paste("game-metadata.csv", sep = "/") %>% 
  read_csv()
```

As you can see in the table below, the metadata we extracted contains for each match the date and starting time, the home team with their respective points and the visitor team with their respective points.

```{r display-game-metadata, eval = TRUE, message = TRUE}
game_metadata %>%
  head(100)
```

### 4.2 Getting the Twitter Data
The Twitter-dataset contains all tweets related to the players we want to inspect in our analysis for their on-court performance and this part of the notebook contains the whole pipeline of extracting these relevant tweets. But before we were able to exploit the API, some pre-work had to be done.
First, we set up the main directory for our data to be created and the four datasets we previously created from the [basketball-reference.com](https://basketball-reference.com) source get loaded. In the next step and before we could start with the extraction of relevant tweets for our NBA players, we had to narrow down the number of players to be considered by our pipeline. The datasets above were created over 227 players. As the process of collecting tweets for such a number of individuals would be a huge overload, we decided to pick those top players, which are most relevant for us, following some criteria we set up in the following.

```{r read_nba_data, eval = TRUE}
player_metadata <- paste(data_dir,"player-metadata.csv", sep ="/") %>% read_csv()
player_game_stats <- paste(data_dir,"player-game-stats.csv", sep ="/") %>% read_csv()
game_metadata <- paste(data_dir,"game-metadata.csv", sep ="/") %>% read_csv()
player_season_stats <- paste(data_dir,"player-season-stats.csv", sep ="/") %>% read_csv()
```

#### 4.2.1 Players who played actively for the same team
First of all we picked those players, which continuously played in the regular seasons 2016/17 - 2018/19. We didn't consider the playoffs here, as many players don't get into the playoffs with their teams but still play a full regular season and therefore provide enough interesting game-data for our analysis. Furthermore, we only wanted those players in our dataset, who stayed at their respective team for the whole time of observation. The idea behind this was to eliminate team switches as possible factors that influence the player's performance aside those we wanted to observe. Additionally we considered only those players who had on-court time in at least 80% of the games during their regular season.

#### 4.2.2 Players whose BPM varied by a standard deviation of at least 8
As third parameter we inspected the variable "Box Plus/Minus" (BPM) in the player_game_stats dataset, which is a score-based performance indicator that already was briefly introduced in the section before.
With this estimate, we wanted to extract those players, whose performance is relatively unstable in comparison to their colleagues by computing the standard deviation of performance for each player and storing them from the highest deviation in descending order. On this dataset we applied a cutoff value to get only those players, whose standard deviation was higher or equal to 8. The assumption behind this filter parameter was, that an influence of social media on the performance could only be observed, when there is a change in performance over the whole observation period. On players who have a stable performance, we would not be able to measure an impact on a change if there was no/just very little change.

#### 4.2.3 Players with at least 1.000 followers
The last parameter we wanted to include into our selection concerned about the players who have a minimal follower count of 1.000 users on Twitter. For this we joined our data with the twitter metadata we extracted along with the tweets themselves and addressed the variable 'follower_count' to be at least of size 1.000. Similar to the prior filter condition, the idea behind this was to have only players in consideration, who have possibly enough tweets that could generate an impact on their performance. The assumption: Players with less than 1.000 followers are very likely to don't get a sufficient amount of tweets for our observations. Finally we then filtered our 'relevant_players' tibble by an inner join with this selection on their common variable 'screen_name'.

```{r select_relevant_players, eval = TRUE}
relevant_players <- player_season_stats %>% 
  
  # Players who played actively (>= 80% of games) for the same team between 2016-2019 (3 seasons)
  filter(Season %in% c("2016-17", "2017-18", "2018-19")) %>% 
  filter(SeasonType == "Regular Season") %>% 
  group_by(BBRef_Player_ID) %>% 
  filter(n() == 3) %>% 
  summarise(
    team_cnt = length(unique(Team)),
    game_cnt = sum(G)
  ) %>% 
  filter(team_cnt == 1) %>% 
  filter(game_cnt >= 0.8 * 82 * 3) %>% 
  select(BBRef_Player_ID) %>% 
  
  # Players whose BPM varied by a standard deviation of at least 8
  inner_join(player_game_stats) %>% 
  group_by(BBRef_Player_ID) %>% 
  summarise(sd_BPM = sd(BPM, na.rm = TRUE)) %>% 
  filter(sd_BPM >= 8) %>% 
  select(BBRef_Player_ID) %>% 
  
  # Players with at least 1000 followers
  inner_join(player_metadata) %>% 
  pmap_dfr(function(...){
    relevant_player <- tibble(...)
    twitter_meta <- relevant_player$Twitter %>% 
      lookup_users(token = twitter_token) %>% 
      select(c("screen_name", "followers_count"))
    inner_join(relevant_player, twitter_meta, by = c("Twitter" = "screen_name"))
  }) %>% filter(followers_count >= 1000) %>% 
  select(player_metadata %>% names())
```

#### 4.2.4 Merging the parameters
Finally we merged the cut-off standard deviation values of the players with their respective twitter-account data, including the count of followers, the count of posted statuses, the count of accounts indicated as favourites and the player's screen name. Now the last step was to create a final set of players we wanted to consider in our analysis by merging the two data sets created into one and picking the top intersecting players.

```{r display_relevant_players, eval = TRUE, message = TRUE}
relevant_players %>%
  head(100)
```

```{r select_relevant_player_stats, eval = TRUE}
relevant_player_stats <- relevant_players %>% 
  inner_join(player_game_stats) %>% 
  filter(! is.na(MP)) %>% 
  inner_join(game_metadata) %>% 
  filter(Season %in% c("2017-18", "2018-19"))
```

```{r display_relevant_player_stats, eval = TRUE, message = TRUE}
relevant_player_stats %>%
  head(100)
```

### 4.3 Extracting the relevant tweets
With the given data we were now able to extract exactly those tweets we needed for our analysis. To do so, we chose to use the get_all_tweets function from the `academictwitteR` packagage.

To only extract tweets that can be assumed to be relevant for a specific game day, we delimited the time range of tweets to be considered for the extraction to the time between 24 hours and 45 minutes before a game (to be on the safe side, we first extracted tweets in a range of 48 hours before a game and boiled it down to 24 hours in an extra step). With the first limit we wanted to avoid that tweets, related to another match, get considered as it is not unusual that one player performs two games in two days. The 45-minute delimiter was set according to the assumption, that it is unlikely for players to check their Twitter-account just 45 minutes before a game (It is even forbidden to players to look on their phones 15 minutes before a game). 
After these parameters were set, we obtained the tweets for each player of our preselection-list discussed before. Alongside with the raw text, the date of creation, the count how often a post was retweeted, the reply count, the count of likes for each tweet and the quote count were added to the dataset of each player's tweets. A very important step for our later analysis was to map each tweet to the respective BBRef_Player_ID and the BBRef_Game_ID, to be able to address tweets based on a player- or a game-selection. Finally, each set got stored under the player's twitter name.

```{r extract_and_save_tweets, eval = FALSE}
#Attention! We set the extract_and_save_tweets code snippet on hold to sace processing effort and to not load millions of tweets for each rendering
#Attention!
time_window <- 48
tweet_cols <- c("BBRef_Player_ID", "BBRef_Game_ID", "id", "text", "created_at", "retweet_count", "reply_count", "like_count", "quote_count")
alrdy_proc_plyrs <- list.files(tweets_dir) %>% map_chr(~ str_remove(.x, "\\.csv$"))


relevant_player_stats %>% 
  filter(! Twitter %in% c(alrdy_proc_plyrs)) %>% 
  mutate(EndTweet = DateTime - lubridate::dminutes(45)) %>% 
  mutate(StartTweet = EndTweet - lubridate::dhours(time_window)) %>% 
  
  # Save tweets player-wise
  group_by(BBRef_Player_ID) %>% 
  group_walk(~ {
    tweets <- .x %>% pmap_dfr(function(...) {
      player_game_data <- tibble(...)
      
      start_tweets = player_game_data$StartTweet %>% iso8601() %>% paste0("Z")
      end_tweets = player_game_data$EndTweet %>% iso8601() %>% paste0("Z")
      
      twts <- tibble()
      
      tryCatch({
          twts <- get_all_tweets(
            query = player_game_data$Twitter,
            start_tweets = start_tweets,
            end_tweets = end_tweets,
            is_retweet = FALSE,
            lang = "en",
            bearer_token = twitter_bearer_token
          ) %>% 
            mutate(BBRef_Player_ID = player_game_data$BBRef_Player_ID) %>% 
            mutate(BBRef_Game_ID = player_game_data$BBRef_Game_ID)
        }, error = function(e) {
          # For Status Code 503
          print("Error loading tweets for the following game: ")
          print(player_game_data %>% select(c(Twitter, BBRef_Game_ID, DateTime, StartTweet, EndTweet)))
        }, finally = {
          return(twts)
        }
      )
      
    }) %>% 
      mutate(retweet_count = public_metrics$retweet_count) %>% 
      mutate(reply_count = public_metrics$reply_count) %>% 
      mutate(like_count = public_metrics$like_count) %>% 
      mutate(quote_count = public_metrics$quote_count) %>% 
      select(tweet_cols)
    
    file_name <- .x$Twitter %>% unique() %>% paste0(".csv")
    file_path <- tweets_dir %>% paste(file_name, sep = "/")
    
    tweets %>% write_csv(file_path)
  })
```

```{r load_tweets_1, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

tweets <- player_metadata %>% inner_join(tweets)
```

```{r display_tweets, eval = TRUE, message = TRUE}
tweets %>%
  head(100)
```

# 5. Pre-processing the Twitter Data
After we finished setting up our datasets and before extracting sentiments from the tweets it was reasonable to pre-process them in advance in order to improve the accuracy of the computed sentiments, especially for our case where the text was in the form of tweets. Tweets are most often written in a less formal language, including abbrevations and slang, so our focus laid in identifying and converting these phenomenons into a language that has increased machine readability. That is why we used the `textclean` package to apply the following pre-processing steps on each tweet: 

At first, each tweet got lowercased. Then we resolved non-ascii characters, replaced html-symbols by word meanings (e.g. "&amp" to "and") so that they can be captured by the analyzer and replaced (multiple succeeding) white space symbols by single white spaces (e.g. "\\t" by " "). After that, Twitter mentions, hashtags as well as URLs (e.g. "@StephenCurry30", "#BBNFOREVER", "https://t.co/37cSfQhMJs") got removed as they give us no further information about the sentiments and only force the analyzer to run over more words to check in the lexicon. Additionally we replaced emojis by their word meaning (e.g. ":)" to "smiley"). Similar to how we treated html-symbols, contractions got replaced by their multi-word forms (e.g. "I'll" to "I will") again for the sake of machine readability. 
We replaced common words, which letters were written with spaces in between (for emphasis), by their semantic equivalent without spaces (e.g. "B O M B" to "BOMB") and replaced word lengthening to emphasize or alter word meanings by their semantic equivalent (e.g. "niiiice" to "nice"). Finally, internet slang and abbreviations got replaced by their semantic equivalent (e.g. "YOLO" to "you only live once").

A very important decision that should be noted is, that stemming (i.e. Porter Stemming) was not applied to the tweets since the terms are written in their base form in the sentiment lexicons. Additionally stopword-removal was not performed to avoid the risk of removing potentially crucial valence shifters for the sentiment extraction (e.g. in "I am **not** happy" the term "**not**" negates the sentiment and should therefore not be removed). These valence shifters will play a significant role in the sentiment analyzer we chose to work on our data and we will get back to this in a later section.

```{r load_tweets_2, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv) %>% 
  select(c(id, text)) %>% 
  
  # Eliminate duplicate tweets (addressed to multiple players) to accelerate processing
  unique() 
```

```{r preprocess_tweets}
prep_tweets <- tweets %>% 
  mutate(prep_text = imap_chr(tweets$text, ~{
    
    print(.y %>% paste(nrow(tweets), sep = "/"))
      
    # Replace words which letters are written with spaces in between for emphasis by their semantic equivalence (e.g. "B O M B" -> "BOMB")
    txt <- replace_kern(.x)
    
    # Replace emoji by description
    txt <- replace_emoji(txt)
    
    # Replace non-ascii characters by semantic equivalent
    txt <- replace_non_ascii(txt)
    
    # Replace contractions by their multi-word forms (e.g. "I'll" -> "I will")
    txt <- replace_contraction(txt)
    
    # Text to lowercase 
    txt <- str_to_lower(txt)
    
    # Remove Twitter URL's, mentions and hashtags
    txt <- replace_url(txt) 
    txt <- replace_tag(txt)
    txt <- replace_hash(txt)
    
    # Replace html symbols by semantic equivalent symbol (e.g. "&amp;" -> "and")
    txt <- replace_html(txt)
    txt <- replace_symbol(txt)
    
    # Replace word lenghenings to emphasize or alter word meanings by their semantic equivalence (e.g. "I said heyyy!" -> "I said hey sexy!")
    txt <- replace_word_elongation(txt, impart.meaning = TRUE) 
    
    # Replace internet slang by their semantic equivalence (e.g. "YOLO" -> "You only live once")
    txt <- replace_internet_slang(txt)
    
    # Replace white space characters by single white space and trim
    txt <- replace_white(txt)
    txt <- str_trim(txt)
    
  }))
  
```

One of the first issues we were confronted with when we started looking into our extracted twitter data and ran a first sentiment analysis on it, was that emojis didn't get processed properly by any analyzer we tested. But in our opinion, no component of a tweet carries emotions so strongly than these emojis (which of course already gets implied by the name). So, besides the pre-processing of the textual information of the tweets themselves, we decided to additionally handle emojis separately by letting a special emoji-analyzer, the **_Novak_** Emoji Sentiment Lexicon, run over them. They were extracted from each tweet and stored by their key representation (from the **_Novak_** Emoji Sentiment Lexicon) in a separate variable separated by white spaces in order to use them for an encapsulated emoji sentiment computation for the individual tweets.

```{r extract_emojis}
emoji_sentiment_lexicon <- lexicon::hash_sentiment_emojis %>% tibble()
emoji_regex <- emoji_sentiment_lexicon$x %>% paste(collapse = "|")

prep_tweets <- prep_tweets %>% 
  mutate(emojis = map_chr(tweets$text, ~ {
    .x %>% 
      replace_emoji_identifier() %>% 
      str_extract_all(emoji_regex) %>% 
      unlist() %>% 
      paste(collapse = " ")
  }))
```

```{r store_preprocessed_tweets}
file_path <- data_dir %>% paste("prep-tweets.csv", sep = "/")

prep_tweets %>% 
  select(c(id, prep_text, emojis)) %>% 
  write_csv(file_path)
```

```{r load_preprocessed_tweets, eval = TRUE}
prep_tweets <-data_dir %>% 
  paste("prep-tweets.csv", sep = "/") %>% 
  read_csv()
```

The following table gives an idea about how the tweets look like before and after the performed pre-processing steps: 

```{r display_preprocessed_tweets, eval = TRUE}
tweets %>%
  inner_join(prep_tweets) %>% 
  select(c(text, prep_text, emojis)) %>% 
  unique() %>%
  head(100)
```

# 6. Sentiment Extraction
Our next task was to extract sentiments from the pre-processed tweets and extracted emojis. To solve this task we used the package `sentimentr`, since compared to other solutions (e.g. `syuzhet` and `tidytext`), `sentimentr` uses an ordered bag of words model that allows it to incorporate valance shifters before or after polarized words to negate or intensify their sentiment (e.g. "I do **not** like it!" or "I **really** like it!"). That ultimately gives `sentimentr` the power to much more accurately assign sentiments to text passages. 

The sentiments were computed sentence-wise for each tweet and aggregated via the ``average_downweighted_zero`` ``sentimentr``-function that downweights sentiment-scores for sentences close to zero. 

The following sentiment lexica were used to compute the sentiments for each tweet by making use of the `lexicon` package: 

* **_Bing_**: positive/negative word list created by Hu Xu and Bing Liu (TODO reference).
* **_Syuzhet_**: word list with sentiment scores reaching from -1 to 1 created by Matthew L. Jockers (TODO reference).
* **_Jockers-Rinker_**: combined version of the Jocker's **_Syuzhet_** lexicon and Rinker's augmented **_Bing_** lexicon (TODO reference).
* **_NRC_**: positive/negative word list created by Saif M. Mohammad (TODO reference).
* **_AFINN_**: word list with sentiments reaching on a discrete scale from -5 to 5 created by Finn Årup Nielsen (TODO reference).
* **_Novak_**: list of emojis with sentiment scores reaching from -1 to 1 created by Kralji Novak (TODO reference http://kt.ijs.si/data/Emoji_sentiment_ranking/index.html). It should be noted that this sentiment lexicon was only applied on the extracted emojis but not on the text of the pre-processed tweets. Furthermore tweets that contained no emojis were excluded from the sentiment computation. 

```{r load_tweets_3, eval = TRUE}
tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

prep_tweets <- data_dir %>% paste("prep-tweets.csv", sep = "/") %>% read_csv()

# tweets %>% inner_join(prep_tweets) %>% select(c(text, prep_text, emojis))
```

```{r bing_sentiments}
bing_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_huliu) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("bing.csv", sep = "/")
bing_sentiments %>% write_csv(file_path)

bing_sentiments
```

```{r syuzhet_sentiments}
syuzhet_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_jockers) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("syuzhet.csv", sep = "/")
syuzhet_sentiments %>% write_csv(file_path)

syuzhet_sentiments
```

```{r jockers_rinker_sentiments}
jockers_rinker_sentiments <- prep_tweets %>%
  mutate(sentences = get_sentences(prep_text)) %$%  
  sentiment_by(sentences, list(id)) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("jockers-rinker.csv", sep = "/", polarity_dt = lexicon::hash_sentiment_jockers_rinker)
jockers_rinker_sentiments %>% write_csv(file_path)

jockers_rinker_sentiments
```

```{r nrc_sentients}
nrc_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_nrc) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("nrc.csv", sep = "/")
nrc_sentiments %>% write_csv(file_path)

nrc_sentiments
```

```{r afinn_sentiments}
# Note: download for "afinn" has to be confirmed
hash_sentiment_afinn <- tidytext::get_sentiments("afinn") %>% 
  rename(c(x = word, y = value)) %>% 
  as_key() # convert tibble to data table for sentimentr

afinn_sentiments <- prep_tweets %>% 
  mutate(sentences = get_sentences(prep_text)) %$%
  sentiment_by(sentences, list(id), polarity_dt = hash_sentiment_afinn) %>% 
  tibble()

file_path <- sentiments_dir %>% paste("afinn.csv", sep = "/")
afinn_sentiments %>% write_csv(file_path)

afinn_sentiments
```

```{r novak_emoji_sentiments}
novak_emoji_sentiments <- prep_tweets %>%
  filter(emojis != "") %>% 
  filter(! is.na(emojis)) %>%
  mutate(emojis = replace_emoji_identifier(emojis)) %>% 
  mutate(sentences = get_sentences(emojis)) %$%
  sentiment_by(sentences, list(id), polarity_dt = lexicon::hash_sentiment_emojis) %>% 
  tibble() 

file_path <- sentiments_dir %>% paste("novak-emoji.csv", sep = "/")
novak_emoji_sentiments %>% write_csv(file_path)

novak_emoji_sentiments 
```

# 7. Exploratory Data Analysis


# 8. Predictive Model
One attempt to check whether the sentiments have an influence on the player's performance at all and therefore to investigate our findings in the correlation analysis, was to set up a random forest regression model to predict the BPM of players. The idea was to set up different models and compare the predictions of performance with and without the sentiments as input features:

 - mean BPM of the last 5 BPMs as a baseline model
 - model including only the sentiment scores
 - model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent
 - model including the last 5 BPMs, Position, Age, Month of the Game, Homegame, Trend, SRS_Team, SRS_Opponent and the sentiment scores
 
In detail, Homegame expresses whether the player played a homegame (1) or an away game (0). The Trend indicates the teams last 5 game performances - i.e. the sum of wins (+1) and losses (-1) is calculated. In order to measure a more longterm team performance we used the SRS (Simple Rating System) which gives a score to each team according to their average point difference and strength of schedule and where 0 marks the average score. The SRS for the previous season was used for the prediction.

We started by loading the tweets and extracted the necessary columns before transforming them. 

```{r load_tweets_and_transform, message = FALSE}
data_dir <- "../data"
sentiments_dir <- "../data/sentiments"
tweets_dir <- data_dir %>% paste("tweets", sep = "/")

tweets <- list.files(tweets_dir, full.names = TRUE) %>% 
  map_dfr(read_csv)

player_game_stats <- data_dir %>% paste("player-game-stats.csv", sep = "/") %>% read_csv()
player_season_stats <- data_dir %>% paste("player-season-stats.csv", sep = "/") %>% read_csv()

games <- tweets %>% 
  select(BBRef_Player_ID ,BBRef_Game_ID) %>%
  unique() %>%
  left_join(player_game_stats %>% select(BBRef_Player_ID,BBRef_Game_ID,Season, Date, Tm, HTm, Opp, WL,BPM), by = c("BBRef_Player_ID" = "BBRef_Player_ID", "BBRef_Game_ID" = "BBRef_Game_ID")) %>%
  left_join(player_season_stats %>% select(BBRef_Player_ID,Season, Age, Pos), by = c("BBRef_Player_ID" = "BBRef_Player_ID", "Season" = "Season")) %>%
  unique()

player_game_stats <- player_game_stats %>%
  mutate(index = 1:nrow(player_game_stats)) %>% # get an index in order to use for the last 5 BPMs
  mutate(WL_ = ifelse(str_detect(player_game_stats$WL, "W"),1,-1)) # use for trend variable

games <- games %>%
  mutate(Homegame = ifelse(games$Tm == games$HTm,1,0)) %>%
  mutate(Month = month(Date))

get_BPM_Trend <- function(player, game){
  index <- player_game_stats %>%
    filter(BBRef_Player_ID == player,BBRef_Game_ID == game) %>%
    select(index) %>%
    pull()
  
  games <- games %>%
    filter(BBRef_Player_ID == player,BBRef_Game_ID == game) %>%
    mutate(BPM_5 = player_game_stats$BPM[index-5]) %>%
    mutate(BPM_4 = player_game_stats$BPM[index-4]) %>%
    mutate(BPM_3 = player_game_stats$BPM[index-3]) %>%
    mutate(BPM_2 = player_game_stats$BPM[index-2]) %>%
    mutate(BPM_1 = player_game_stats$BPM[index-1]) %>%
    mutate(Trend = sum(player_game_stats$WL_[(index-5):(index-1)]))
}

games <- map2_dfr(games$BBRef_Player_ID, games$BBRef_Game_ID, ~ get_BPM_Trend(.x, .y)) # get the last 5 BPMs and Trend for each player/game combination
```

After that the SRS (Simple Rating System) for the previous season ot extracted. This was done by parsing the html table from [basketball-reference.com](https://basketball-reference.com) and using the teams shortcut to combine the data with the previous dataset.

```{r load_basketball_reference_data, message = FALSE}
content <- read_html("https://www.basketball-reference.com/leagues/NBA_2018.html")
tables <- content %>% html_table(fill = TRUE)
team_stats_2018 <- tables[[11]]

team_stats_2018 <- team_stats_2018[,c(2,10)] %>% 
  tail(-1) %>% 
  head(-1) %>%
  setNames(c("Team","SRS")) %>%
  mutate(SRS = as.double(SRS)) %>%
  mutate(Team_name = c("HOU","TOR","GSW","UTA","PHI","BOS","OKC","SAS","POR","MIN","DEN","IND","NOP","CLE","WAS","MIA","CHO","LAC","DET","MIL","LAL","DAL","NYK","BRK","ORL","ATL","MEM","CHI","SAC","PHO")) %>%
  mutate(Season_plus_one = "2018-19")

content <- read_html("https://www.basketball-reference.com/leagues/NBA_2017.html")
tables <- content %>% html_table(fill = TRUE)
team_stats_2017 <- tables[[11]]

team_stats_2017 <- team_stats_2017[,c(2,10)] %>% 
  tail(-1) %>% 
  head(-1) %>%
  setNames(c("Team","SRS")) %>%
  mutate(SRS = as.double(SRS)) %>%
  mutate(Team_name = c("GSW","SAS","HOU","TOR","LAC","UTA","CLE","BOS","WAS","MIA","OKC","MEM","DEN","CHI","CHO","IND","MIL","POR","ATL","DET","MIN","NOP","DAL","NYK","SAC","PHO","PHI","BRK","ORL","LAL")) %>%
  mutate(Season_plus_one = "2017-18")

team_stats <- rbind(team_stats_2017,team_stats_2018)

games <- games %>% 
  left_join(team_stats, by = c("Season" = "Season_plus_one", "Tm" = "Team_name")) %>%
  left_join(team_stats, by = c("Season" = "Season_plus_one", "Opp" = "Team_name"))
```

Then some of the variables got renamed and the mean BPM of the last 5 BPMs got calculated.

```{r rename_columns, message = FALSE}
data <- games %>%
  select(BBRef_Player_ID,BBRef_Game_ID,Age,Pos,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,Homegame, Month,SRS_Tm = SRS.x , SRS_Opp = SRS.y, BPM, Trend) %>%
  mutate(mean_BPM = rowMeans(games %>%select(BPM_5,BPM_4,BPM_3,BPM_2,BPM_1),na.rm = TRUE)) 
```

In the last pre-processing step, the already computed sentiments got merged with the newly created data.

```{r combine_sentiments_with_data, message = FALSE}
prep_tweets <- data_dir %>% paste("prep-tweets.csv", sep = "/") %>% read_csv()
bing <- sentiments_dir %>% 
  paste("bing.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, word_count, ave_sentiment)) %>% 
  rename(bing = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(bing < 0.0, "negative", "positive"))

syuzhet <- sentiments_dir %>% 
  paste("syuzhet.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(syuzhet = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(syuzhet < 0.0, "negative", "positive"))

jockers_rinker <- sentiments_dir %>% 
  paste("jockers-rinker.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(jockers_rinker = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(jockers_rinker < 0.0, "negative", "positive"))

afinn <- sentiments_dir %>% 
  paste("afinn.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(afinn = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(afinn < 0.0, "negative", "positive"))

nrc <- sentiments_dir %>% 
  paste("nrc.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(nrc = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(nrc < 0.0, "negative", "positive"))

novak_emoji <- sentiments_dir %>% 
  paste("novak-emoji.csv", sep = "/") %>% 
  read_csv() %>% 
  select(c(id, ave_sentiment)) %>% 
  rename(novak_emoji = ave_sentiment) %>%
  mutate(binary_sentiment = if_else(novak_emoji < 0.0, "negative", "positive"))

sentiments <- tweets %>% 
  inner_join(prep_tweets) %>% 
  select(c(BBRef_Player_ID,BBRef_Game_ID,id, text, prep_text)) %>% 
  inner_join(bing) %>%
  inner_join(syuzhet) %>% 
  inner_join(jockers_rinker) %>% 
  inner_join(nrc) %>% 
  inner_join(afinn) %>% 
  inner_join(novak_emoji)

aggregated_sentiments <- sentiments %>% 
  group_by(BBRef_Player_ID, BBRef_Game_ID) %>% 
  summarize(bing = mean(bing), syuzhet = mean(syuzhet), jockers_rinker = mean(jockers_rinker), 
            nrc = mean(nrc), afinn = mean(afinn), novak_emoji = mean(novak_emoji))

data <- data %>% 
  left_join(aggregated_sentiments, by = c("BBRef_Player_ID" = "BBRef_Player_ID", "BBRef_Game_ID" = "BBRef_Game_ID"))


data <- data %>% drop_na(BPM,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,bing,syuzhet,jockers_rinker,nrc,afinn)
```

```{r plot_BPM, message = FALSE}
data %>%
  ggplot(aes(fct_reorder(BBRef_Player_ID,BPM), 
             BPM)) +
  geom_boxplot(outlier.alpha = 0.5) +
  coord_flip() +
  xlab("BPM performance") +
  ylab("BBRef_Player_ID")
```

For the actual prediction task we started by selecting the relevant columns for the model and split the data into training and testing data with a proportion of 75% training to 25% testing. A validation split of 20% of the training set was then performed data in order to combat overfitting. Then each model was created using the recipe package. Using tune_grid(), the model parameter mtry (number of predictors that will be randomly sampled at each split when creating the tree models), trees (number of trees contained in the ensemble) and min_n (minimum number of data points in a node that are required for the node to be split further) got tuned. After tuning, the best model fit got choosen based on the RMSE (Root Mean Square Error) which calculates the average distance between the predicted values and the actual values, i.e. the lower the RMSE, the better the model is able to fit a dataset. The importance score for each variable was saved as well.

```{r predictive_model, message = FALSE}
data <- data %>%
  select(Pos, Age, BPM_5, BPM_4, BPM_3, BPM_2, BPM_1, Homegame, Month, SRS_Tm, SRS_Opp, Trend, bing, syuzhet, jockers_rinker, nrc, afinn, novak_emoji, BPM, mean_BPM) %>% 
  drop_na(BPM,BPM_5,BPM_4,BPM_3,BPM_2,BPM_1,bing,syuzhet,jockers_rinker,nrc,afinn)

set.seed(123)
data_split <- initial_split(data)
data_train <- training(data_split)
data_test <- testing(data_split)
val_set <- validation_split(data_train, prop = 0.8)

# normal model
rf_rec <- recipe(BPM ~ BPM_5 + BPM_4 + BPM_3 + BPM_2 + BPM_1 + Pos + Age + Month + Homegame + Trend + SRS_Tm +  SRS_Opp, data = data_train) 

# model with sentiments
rf_rec_with_sentiment <- recipe(BPM ~ BPM_5 + BPM_4 + BPM_3 + BPM_2 + BPM_1 + Pos + Age + Month + Homegame + Trend + SRS_Tm +  SRS_Opp + bing + syuzhet + jockers_rinker + nrc + afinn + novak_emoji, data = data_train) 

# model with mean BPM
rf_rec_mean <- recipe(BPM ~ mean_BPM, data = data_train)

# model sentiments only
rf_rec_only_sentiment <- recipe(BPM ~ bing + syuzhet + jockers_rinker + nrc + afinn + novak_emoji, data = data_train)


get_model <- function(rec){
  rf_spec <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")
  
  rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rec)  

  rf_res <- rf_wf %>%
  tune_grid(val_set,
            grid = 100,
            control = control_grid(save_pred = TRUE))  
  
  rf_best <- rf_res %>%
  select_best(metric = "rmse")
  
  last_rf_spec <- rand_forest(mtry = rf_best$mtry[1], min_n = rf_best$min_n[1], trees = rf_best$trees[1]) %>%
  set_engine("ranger", importance ="permutation") %>%
  set_mode("regression")

last_rf_wf <- rf_wf %>%
  update_model(last_rf_spec)

last_rf_fit <- last_rf_wf %>%
  last_fit(data_split)
}

model <- get_model(rf_rec)
model_with_sentiment <- get_model(rf_rec_with_sentiment)
model_mean_BPM <- get_model(rf_rec_mean)
model_only_sentiment <- get_model(rf_rec_only_sentiment)
```

In order to determine which model predicted the BPM performance best, a visualization is shown below that displays the predicted value (y-axis) and the true value (x-axis) for each model, meaning a perfectly fitted model would have all predictions on the the 45° line. The visualization doesn't allow for a clear interpretation since all models are scattered and no specific trend can be observed.

```{r predictions, message = FALSE}
predictions <- model$.predictions[[1]] %>%
  mutate(model = "without sentiments") %>%
  bind_rows(model_with_sentiment$.predictions[[1]] %>%
              mutate(model = "with sentiments")) %>%
  bind_rows(model_mean_BPM$.predictions[[1]] %>%
              mutate(model = "mean BPM")) %>%
  bind_rows(model_only_sentiment$.predictions[[1]] %>%
              mutate(model = "sentiments only"))

predictions %>%
  ggplot(aes(BPM, .pred, color = model)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.3) +
  facet_wrap(~ model) + 
  theme(legend.position="none") +
  labs(
      title = "Predicted BPM values vs. true BPM values",
      subtitle = "Calculated for each model", 
      y = "predicted BPM"
    ) + 
  theme(
      plot.title = element_text(hjust = 0.5), 
      plot.subtitle = element_text(hjust = 0.5)
    ) 
```

```{r metrics, message = FALSE}
metrics <- model$.metrics[[1]] %>%
  mutate(model = "without sentiments") %>%
  bind_rows(model_with_sentiment$.metrics[[1]] %>%
              mutate(model = "with sentiments")) %>%
  bind_rows(model_mean_BPM$.metrics[[1]] %>%
              mutate(model = "mean BPM")) %>%
  bind_rows(model_only_sentiment$.metrics[[1]] %>%
              mutate(model = "sentiments only"))

metrics %>%
  select(.metric, .estimate, model)

metrics %>%
  select(.metric, .estimate, model) %>%
  ggplot(aes(x = .estimate, y = fct_reorder(model,.estimate))) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  facet_wrap(~ .metric, scales = "free") +
  geom_text(aes(label=round(.estimate,4)), position=position_dodge(width=0.9), vjust=-0.25) + 
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1)) + 
  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +
  labs(
      title = "RMSE (Root Mean Square Error) and RSQ (R-Squared) ",
      subtitle = "Calculated for each model"
    ) + 
  theme(
      plot.title = element_text(hjust = 0.5), 
      plot.subtitle = element_text(hjust = 0.5)
    )
```

Therefore, to determine which model predicted the values best, the RMSE and RSQ (R-Squared: proportion of variance in the dependent variable that can be explained by the independent variables) was used to further analyze the different models.
The bar plot shows that the RMSE are in the range of `r metrics %>% filter(.metric == "rmse") %>% select(.estimate) %>% min() %>% round(2)` - `r metrics %>% filter(.metric == "rmse") %>% select(.estimate) %>% max() %>% round(2)`. Using RMSE as a metric the model without sentiment scores predicts the BPM best. The difference between the models's RSQ on the other hand is quite large. Here, all models don't explain the BPM well ranging from `r metrics %>% filter(.metric == "rsq") %>% select(.estimate) %>% min()` to `r metrics %>% filter(.metric == "rsq") %>% select(.estimate) %>% min()`. The models including sentiments predict the BPM performance best, directly followed by the model without sentiments.
So even here, a direct influence of the sentiments as a feature can not be derived from the model, underlining our findings in the correlation analysis that the sentiments have no significant influence on a player's in-game performance.

```{r variable_importance, message = FALSE}
get_variable_importance <- function(x){
  list <- x %>%
  pluck(".workflow", 1) %>%
  pull_workflow_fit()
  
  variable <- list$fit$variable.importance %>% names()
  value <- list$fit$variable.importance %>% unlist() %>% unname()

  data.frame(variable, value) %>%
    mutate(sentiment = ifelse(variable %in% c("bing", "syuzhet", "jockers_rinker", "nrc", "afinn", "novak_emoji"),T,F)) %>%
    ggplot(aes(x = fct_reorder(variable,value), y = value, fill = sentiment)) +
    geom_bar(stat= "identity") +
    coord_flip() +
    theme(legend.position="none") +
    xlab("feature") +
    ylab("importance for prediction") +
    scale_fill_manual(values = c("#595959", "#DB2C2C"))+
  labs(
      title = "Variable Importance Scores",
      subtitle = paste("Model:",name, sep=" ")
    ) + 
  theme(
      plot.title = element_text(hjust = 0.5), 
      plot.subtitle = element_text(hjust = 0.5)
    ) 
} 

get_variable_importance(model, "without sentiments")
get_variable_importance(model_with_sentiment, "with sentiments")
get_variable_importance(model_only_sentiment, "sentiments only")
```

A last step we did was to look at the importance scores of each sentiment analyzer method for the model including sentiments. Here it can be observed that the syuzhet and jockers_rinker sentiment scores rank first and second. The (permutation) importance score is calculated by (1) measuring a basline RSQ , (2) permuting the values of one feature and measure the RSQ. The importance score is the difference between the basline and the drop in overall RSQ caused by the permutation.
The plot indicates a high influence of sentiment scores, but since the model has a low RSQ, this should be perceived carefully. To further understand the influence of sentiment scores, a new model should be deployed in further research with the goal to increase the RSQ. 

# 9. Conclusion and Further Considerations
## 9.1 Wrapping It Up
To the end of our project it is time to dedicate a section to the final results and findings of the whole work. It turned out that the most consuming tasks were not only to run the exploratory analyses on our data but also to gather and preprocess the data itself. Especially the processing of the twitter data has held some unexpected challenges, namely the proper handling of emojis, which we solved with an own sentiment lexicon, and the overall handling of the Twitter API, which turned out to be more complex than expected. Nonetheless we were able to generate meaningful data, that contains all variables needed to run some interesting analyses on it. Especially Twitter offered a wide range of metadata that got delivered with each tweet (e.g. the retweet count, which was vital for one of our correlation approaches).\newlines

With the exploratory data analysis we wanted to answer our initial research questions. For this purpose we ran different approaches over the data to check, whether there is a significant correlation between the average sentiment of tweets a player receives before a game and his performance in-game. Unfortunately, it is to say, that our hypothesis doesn't hold and there is no such significance to be observed. Therefore the conclusion of our analysis at this point is that the Null-hypothesis holds true.\newline

To further investigate the findings we made, we wanted to elaborate the impact of the sentiments on a prediction model that predicts the players BPM performance score. The idea: If the sentiments highly contribute to the prediction and this prediction then is relatively good, it could be interpreted as indicator, that the significant correlation exists after all and we just made some mistakes in the analysis setup. And indeed did the majority of the sentiment scores highly contribute to the prediction. But unfortunately the prediction was quite poor. This could be interpreted as the sentiments pushing the predictor into a wrong learning direction and therefore are not significantly correlated to the prediction outcome.

## 9.2. Further Considerations
Reflecting the overall project and its outcome some consideration regarding the whole project setup can be done:\newline
1. For the twitter data, domain specific and more advanced sentiment extraction methods could be found or existing analyzers be tuned
2. For the prediction model, the input variables should be reviewed to gain a better prediction outcome
3. Generally the correlation analysis could be decoupled from the BPM performance variable and be applied to component variables of the BMP score (e.g. a correlation between the sentiments and the 3-point-field-goal percentage)
4. Further assumptions could be incorporated into the process, like the fact that some players don't manage their own twitter accounts at all but let professional social-media agencies monitor the activities. Such accounts are of course irrelevant for our analysis